{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lesson 6: ADK + MCP Integration - The Complete AI Agent Stack\n\n## ⚠️  IMPORTANT: LOCAL EXECUTION REQUIRED\n\n**This notebook requires LOCAL execution** (Jupyter Lab, VS Code, PyCharm, etc.)\n\n**Why not Colab?**  \nMCP servers communicate via stdio (standard input/output), which is not compatible with Google Colab's custom I/O system. You'll encounter `UnsupportedOperation: fileno` errors if you try to run this in Colab.\n\n**Options:**\n1. **Run locally** - Download this notebook and run on your machine (recommended)\n2. **Read & Learn** - Study the architecture patterns without executing\n3. **Adapt** - Modify to use HTTP-based MCP servers (advanced)\n\n---\n\n## 🎯 Learning Objectives\n\nBy the end of this lesson, you will be able to:\n\n1. **Understand** the modern AI agent architecture stack\n2. **Create** MCP servers that expose data and tools\n3. **Connect** ADK agents to MCP servers using MCPToolset\n4. **Build** production-grade systems combining ADK orchestration + MCP data connectivity\n5. **Design** composable, modular AI architectures\n6. **Deploy** the complete stack locally\n\n## 🎓 Course Journey: Where We Are Now\n\n**Lessons 1-5: ADK Mastery** (✅ Colab-compatible)\n- ✅ Basic agents with LLMs\n- ✅ Function calling and tools\n- ✅ Multi-agent coordination\n- ✅ Workflow patterns (Sequential, Parallel, Loop)\n- ✅ Hybrid architectures (rules + LLMs)\n\n**Lesson 6: The Complete Stack** (⚙️ Local execution required)\n- 🚀 Bringing it all together!\n- 🔗 Connecting ADK to real data sources via MCP\n- 🏗️ Building production-ready architectures\n\n---\n\n## 🏗️ The Modern AI Agent Stack\n\n### Three-Layer Architecture\n\n```\n┌─────────────────────────────────────────────┐\n│  Layer 1: LLM Provider (OpenAI)            │  ← Reasoning & Intelligence\n│  - gpt-5-nano, gpt-4o, etc.                │\n│  - Model-agnostic (swap providers easily)  │\n└─────────────────────────────────────────────┘\n                    ▲\n                    │\n┌─────────────────────────────────────────────┐\n│  Layer 2: ADK (Agent Development Kit)      │  ← Orchestration & Coordination\n│  - Multi-agent coordination                │\n│  - Workflow patterns                       │\n│  - Tool management                         │\n│  - Session handling                        │\n└─────────────────────────────────────────────┘\n                    ▲\n                    │\n┌─────────────────────────────────────────────┐\n│  Layer 3: MCP (Model Context Protocol)     │  ← Data Connectivity\n│  - Standardized tool/data interface        │\n│  - Connect to: databases, APIs, files      │\n│  - Reusable across AI frameworks          │\n└─────────────────────────────────────────────┘\n                    ▲\n                    │\n┌─────────────────────────────────────────────┐\n│  Data Sources                              │\n│  - Ticket DB, Knowledge Base, Monitoring   │\n└─────────────────────────────────────────────┘\n```\n\n### Why This Architecture?\n\n**Separation of Concerns:**\n- 🧠 **LLM**: Provides reasoning (model-agnostic)\n- 🎭 **ADK**: Orchestrates agents and workflows\n- 🔌 **MCP**: Standardizes data connectivity\n\n**Benefits:**\n- ✅ **Composability**: Mix and match components\n- ✅ **Reusability**: MCP servers work with any MCP client\n- ✅ **Maintainability**: Change one layer without affecting others\n- ✅ **Scalability**: Add new data sources easily\n\n### Real-World Use Case: IT Support System\n\nWe'll build a complete IT support system:\n\n**MCP Servers (Data Layer):**\n1. 🎫 **Ticket Database Server**: CRUD operations for support tickets\n2. 📚 **Knowledge Base Server**: Search documentation and articles\n3. 🖥️ **System Monitoring Server**: Check system health and metrics\n\n**ADK Agents (Orchestration Layer):**\n1. 🤖 **Support Coordinator**: Routes requests to specialists\n2. 🎫 **Ticket Specialist**: Manages tickets via MCP\n3. 📖 **Knowledge Specialist**: Searches docs via MCP\n4. 🔧 **System Specialist**: Monitors systems via MCP\n\n**OpenAI (Reasoning Layer):**\n- All agents use gpt-5-nano for decision-making\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Part 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install -q google-adk litellm openai python-dotenv nest-asyncio mcp httpx\n\nprint(\"✅ Packages installed successfully!\")\nprint(\"   - google-adk: Agent orchestration\")\nprint(\"   - mcp: Model Context Protocol\")\nprint(\"   - litellm: OpenAI integration\")\nprint(\"\")\nprint(\"⚠️  IMPORTANT NOTE ABOUT GOOGLE COLAB:\")\nprint(\"   MCP servers use stdio (standard input/output) for communication.\")\nprint(\"   This does NOT work in Google Colab due to Colab's custom I/O handling.\")\nprint(\"\")\nprint(\"   This notebook is designed for LOCAL execution (Jupyter, VS Code, etc.)\")\nprint(\"   To run in Colab, you would need to use HTTP-based MCP servers instead.\")\nprint(\"\")\nprint(\"   For learning purposes, you can:\")\nprint(\"   1. Run this notebook locally (recommended)\")\nprint(\"   2. Read through the code to understand the architecture\")\nprint(\"   3. Adapt for HTTP-based MCP servers for Colab\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Core ADK imports\nfrom google.adk.agents import LlmAgent\nfrom google.adk.runners import Runner\nfrom google.adk.sessions import InMemorySessionService\nfrom google.adk.models.lite_llm import LiteLlm\nfrom google.genai import types\n\n# MCP integration imports\nfrom google.adk.tools.mcp_tool.mcp_toolset import MCPToolset\nfrom google.adk.tools.mcp_tool.mcp_session_manager import StdioConnectionParams\nfrom mcp import StdioServerParameters\n\n# System imports\nimport os\nimport sys\nimport asyncio\nimport subprocess\nimport time\nimport signal\nfrom pathlib import Path\n\n# Enable nested asyncio for Colab\nimport nest_asyncio\nnest_asyncio.apply()\n\nprint(\"✅ Imports successful!\")\nprint(\"   Key import: MCPToolset for connecting to MCP servers\")\nprint(\"   MCP integration: google.adk.tools.mcp_tool\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure OpenAI API key\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"✅ API key loaded from Colab secrets\")\n",
    "except:\n",
    "    from getpass import getpass\n",
    "    print(\"💡 To use Colab secrets: Go to 🔑 (left sidebar) → Add new secret → Name: OPENAI_API_KEY\")\n",
    "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == \"\":\n",
    "    raise ValueError(\"❌ ERROR: No API key provided!\")\n",
    "\n",
    "print(\"✅ Authentication configured!\")\n",
    "\n",
    "# Model configuration\n",
    "OPENAI_MODEL = \"gpt-5-nano\"  # Using gpt-5-nano for cost efficiency\n",
    "\n",
    "print(f\"\\n🤖 Model: {OPENAI_MODEL}\")\n",
    "print(f\"🏗️  Architecture: OpenAI (reasoning) + ADK (orchestration) + MCP (data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📦 Part 2: Setting Up MCP Servers\n",
    "\n",
    "### What is MCP?\n",
    "\n",
    "**Model Context Protocol (MCP)** is an open standard for connecting AI systems to data sources.\n",
    "\n",
    "**Key Concepts:**\n",
    "- 🔧 **Tools**: Functions the AI can call (like ADK tools)\n",
    "- 📚 **Resources**: Data the AI can read (files, databases)\n",
    "- 💬 **Prompts**: Pre-defined prompt templates\n",
    "\n",
    "**MCP Server Communication:**\n",
    "- Uses **stdio** (standard input/output) transport\n",
    "- Server runs as separate process\n",
    "- Client (ADK) communicates via JSON-RPC\n",
    "\n",
    "### Our MCP Servers\n",
    "\n",
    "We've created 3 MCP servers for IT support:\n",
    "\n",
    "1. **ticket_mcp_server.py**\n",
    "   - Tools: `get_ticket`, `list_tickets`, `create_ticket`, `update_ticket`, `search_tickets`\n",
    "   - Data: In-memory ticket database\n",
    "\n",
    "2. **knowledge_mcp_server.py**\n",
    "   - Tools: `search_knowledge_base`, `get_article`, `list_articles`, `get_popular_articles`\n",
    "   - Data: IT support documentation and guides\n",
    "\n",
    "3. **system_monitoring_mcp_server.py**\n",
    "   - Tools: `check_system_health`, `list_all_systems`, `get_system_metrics`, `ping_system`, `get_system_logs`, `get_alerts`\n",
    "   - Data: System health and monitoring data\n",
    "\n",
    "### Colab Challenge: Running Background Servers\n",
    "\n",
    "In Google Colab, we need to:\n",
    "1. Download MCP server files\n",
    "2. Start servers as background processes\n",
    "3. Keep them running while we interact with agents\n",
    "4. Clean up processes when done\n",
    "\n",
    "Let's set this up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: Download MCP Server Files\n",
    "\n",
    "We'll download the pre-built MCP servers from our repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Colab: Download MCP server files\n",
    "# If you're running locally, ensure these files are in the same directory as this notebook\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "# GitHub raw file URLs (update these to your actual repository)\n",
    "BASE_URL = \"https://raw.githubusercontent.com/YOUR_USERNAME/ADK-tutorial/main/\"\n",
    "\n",
    "MCP_SERVERS = [\n",
    "    \"ticket_mcp_server.py\",\n",
    "    \"knowledge_mcp_server.py\",\n",
    "    \"system_monitoring_mcp_server.py\"\n",
    "]\n",
    "\n",
    "# Check if files exist locally first (for local development)\n",
    "try:\n",
    "    # Try to import from current directory\n",
    "    import ticket_mcp_server\n",
    "    print(\"✅ MCP server files found locally\")\n",
    "except ImportError:\n",
    "    print(\"📥 Downloading MCP server files...\")\n",
    "    # In Colab, download from GitHub\n",
    "    # For this tutorial, we'll create them inline\n",
    "    print(\"⚠️  For production, download from: \" + BASE_URL)\n",
    "    print(\"💡 For this tutorial, MCP servers are in the same directory\")\n",
    "\n",
    "# Verify files exist\n",
    "for server_file in MCP_SERVERS:\n",
    "    if Path(server_file).exists():\n",
    "        print(f\"   ✅ {server_file}\")\n",
    "    else:\n",
    "        print(f\"   ⚠️  {server_file} not found - ensure it's in the working directory\")\n",
    "\n",
    "print(\"\\n💡 MCP Servers ready to start!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: MCP Server Management\n",
    "\n",
    "We'll create a helper class to manage MCP server lifecycle in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCPServerManager:\n",
    "    \"\"\"\n",
    "    Manages MCP server processes in Google Colab.\n",
    "    Handles starting, monitoring, and stopping servers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.processes = {}\n",
    "    \n",
    "    def start_server(self, name: str, script_path: str, wait_time: float = 2.0):\n",
    "        \"\"\"\n",
    "        Start an MCP server as a background process.\n",
    "        \n",
    "        Args:\n",
    "            name: Friendly name for the server\n",
    "            script_path: Path to the Python script\n",
    "            wait_time: Seconds to wait for server startup\n",
    "        \"\"\"\n",
    "        if name in self.processes:\n",
    "            print(f\"⚠️  {name} already running (PID: {self.processes[name].pid})\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            print(f\"🚀 Starting {name}...\")\n",
    "            \n",
    "            # Start the server process\n",
    "            process = subprocess.Popen(\n",
    "                [sys.executable, script_path],\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.PIPE,\n",
    "                stdin=subprocess.PIPE\n",
    "            )\n",
    "            \n",
    "            # Store process\n",
    "            self.processes[name] = process\n",
    "            \n",
    "            # Wait for server to start\n",
    "            time.sleep(wait_time)\n",
    "            \n",
    "            # Check if still running\n",
    "            if process.poll() is None:\n",
    "                print(f\"   ✅ {name} started (PID: {process.pid})\")\n",
    "            else:\n",
    "                # Process died\n",
    "                stdout, stderr = process.communicate()\n",
    "                print(f\"   ❌ {name} failed to start\")\n",
    "                if stderr:\n",
    "                    print(f\"   Error: {stderr.decode()[:200]}\")\n",
    "                del self.processes[name]\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Failed to start {name}: {str(e)}\")\n",
    "    \n",
    "    def is_running(self, name: str) -> bool:\n",
    "        \"\"\"Check if a server is still running.\"\"\"\n",
    "        if name not in self.processes:\n",
    "            return False\n",
    "        return self.processes[name].poll() is None\n",
    "    \n",
    "    def get_server_status(self) -> dict:\n",
    "        \"\"\"Get status of all servers.\"\"\"\n",
    "        status = {}\n",
    "        for name, process in self.processes.items():\n",
    "            status[name] = {\n",
    "                \"running\": process.poll() is None,\n",
    "                \"pid\": process.pid\n",
    "            }\n",
    "        return status\n",
    "    \n",
    "    def stop_server(self, name: str):\n",
    "        \"\"\"Stop a specific server.\"\"\"\n",
    "        if name not in self.processes:\n",
    "            print(f\"⚠️  {name} not found\")\n",
    "            return\n",
    "        \n",
    "        process = self.processes[name]\n",
    "        \n",
    "        if process.poll() is None:\n",
    "            print(f\"🛑 Stopping {name} (PID: {process.pid})...\")\n",
    "            process.terminate()\n",
    "            try:\n",
    "                process.wait(timeout=3)\n",
    "                print(f\"   ✅ {name} stopped\")\n",
    "            except subprocess.TimeoutExpired:\n",
    "                print(f\"   ⚠️  {name} didn't stop gracefully, forcing...\")\n",
    "                process.kill()\n",
    "                process.wait()\n",
    "        \n",
    "        del self.processes[name]\n",
    "    \n",
    "    def stop_all(self):\n",
    "        \"\"\"Stop all running servers.\"\"\"\n",
    "        print(\"🛑 Stopping all MCP servers...\")\n",
    "        for name in list(self.processes.keys()):\n",
    "            self.stop_server(name)\n",
    "        print(\"✅ All servers stopped\")\n",
    "    \n",
    "    def __del__(self):\n",
    "        \"\"\"Cleanup on deletion.\"\"\"\n",
    "        self.stop_all()\n",
    "\n",
    "# Create global server manager\n",
    "server_manager = MCPServerManager()\n",
    "\n",
    "print(\"✅ MCP Server Manager ready!\")\n",
    "print(\"   Use: server_manager.start_server(name, script_path)\")\n",
    "print(\"   Use: server_manager.stop_all() when done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: Start MCP Servers\n",
    "\n",
    "Now let's start all three MCP servers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start all MCP servers\n",
    "print(\"🚀 Starting MCP Servers...\\n\")\n",
    "\n",
    "server_manager.start_server(\n",
    "    name=\"Ticket Database\",\n",
    "    script_path=\"ticket_mcp_server.py\",\n",
    "    wait_time=2.0\n",
    ")\n",
    "\n",
    "server_manager.start_server(\n",
    "    name=\"Knowledge Base\",\n",
    "    script_path=\"knowledge_mcp_server.py\",\n",
    "    wait_time=2.0\n",
    ")\n",
    "\n",
    "server_manager.start_server(\n",
    "    name=\"System Monitoring\",\n",
    "    script_path=\"system_monitoring_mcp_server.py\",\n",
    "    wait_time=2.0\n",
    ")\n",
    "\n",
    "# Check status\n",
    "print(\"\\n📊 Server Status:\")\n",
    "status = server_manager.get_server_status()\n",
    "for name, info in status.items():\n",
    "    status_emoji = \"✅\" if info[\"running\"] else \"❌\"\n",
    "    print(f\"   {status_emoji} {name}: {'Running' if info['running'] else 'Stopped'} (PID: {info['pid']})\")\n",
    "\n",
    "print(\"\\n💡 MCP servers are running in the background!\")\n",
    "print(\"   They will automatically stop when the notebook kernel is restarted.\")\n",
    "print(\"   To manually stop: server_manager.stop_all()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 🔌 Part 3: Connecting ADK Agents to MCP Servers\n\n### The MCPToolset Class\n\nADK's `MCPToolset` makes it easy to connect agents to MCP servers:\n\n```python\nfrom google.adk.tools.mcp_tool.mcp_toolset import MCPToolset\nfrom google.adk.tools.mcp_tool.mcp_session_manager import StdioConnectionParams\nfrom mcp import StdioServerParameters\n\n# Create toolset for an MCP server\nticket_tools = MCPToolset(\n    connection_params=StdioConnectionParams(\n        server_params=StdioServerParameters(\n            command=\"python\",\n            args=[\"ticket_mcp_server.py\"]\n        )\n    )\n)\n\n# Add to agent's tools\nagent = LlmAgent(\n    model=LiteLlm(model=\"openai/gpt-5-nano\"),\n    tools=[ticket_tools],  # Agent can now use all ticket DB tools!\n    ...\n)\n```\n\n**Key Components:**\n- `StdioConnectionParams`: Connection configuration for stdio transport\n- `StdioServerParameters`: Server launch parameters (command + args)\n- `command`: Usually \"python\" for Python MCP servers, \"npx\" for Node.js servers\n- `args`: List of arguments including the server script path\n\n**What MCPToolset Does:**\n1. 🔗 Connects to MCP server via stdio\n2. 🔍 Discovers available tools from server\n3. 🔄 Converts MCP tools to ADK tools\n4. 📞 Proxies tool calls to the server\n5. 📦 Returns results to the agent\n\n### Creating Specialist Agents"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Ticket Management Specialist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create MCPToolset for ticket database\nticket_toolset = MCPToolset(\n    connection_params=StdioConnectionParams(\n        server_params=StdioServerParameters(\n            command=\"python\",\n            args=[\"ticket_mcp_server.py\"]\n        )\n    )\n)\n\n# Create ticket specialist agent\nticket_specialist = LlmAgent(\n    model=LiteLlm(model=f\"openai/{OPENAI_MODEL}\"),\n    name=\"ticket_specialist\",\n    tools=[ticket_toolset],\n    instruction=\"\"\"\n    You are a Ticket Management Specialist for IT support.\n    \n    YOUR CAPABILITIES:\n    You can access the ticket database through MCP server tools:\n    - Get ticket details by ID\n    - List and filter tickets (by status, priority, team)\n    - Create new tickets\n    - Update ticket status and details\n    - Search tickets by keywords\n    \n    YOUR ROLE:\n    - Help users find and manage support tickets\n    - Create tickets for new issues\n    - Update ticket status as issues are resolved\n    - Provide ticket summaries and analytics\n    \n    Always use the available tools to access real ticket data.\n    Be concise and helpful in your responses.\n    \"\"\"\n)\n\nprint(\"✅ Ticket Specialist Agent created!\")\nprint(f\"   Model: {OPENAI_MODEL}\")\nprint(f\"   Tools: Connected to Ticket Database MCP Server\")\nprint(f\"   Capabilities: CRUD operations on tickets\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Knowledge Base Specialist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create MCPToolset for knowledge base\nknowledge_toolset = MCPToolset(\n    connection_params=StdioConnectionParams(\n        server_params=StdioServerParameters(\n            command=\"python\",\n            args=[\"knowledge_mcp_server.py\"]\n        )\n    )\n)\n\n# Create knowledge specialist agent\nknowledge_specialist = LlmAgent(\n    model=LiteLlm(model=f\"openai/{OPENAI_MODEL}\"),\n    name=\"knowledge_specialist\",\n    tools=[knowledge_toolset],\n    instruction=\"\"\"\n    You are a Knowledge Base Specialist for IT support.\n    \n    YOUR CAPABILITIES:\n    You can access the knowledge base through MCP server tools:\n    - Search documentation by keywords\n    - Retrieve specific articles by ID\n    - List articles by category\n    - Find popular/helpful articles\n    \n    YOUR ROLE:\n    - Help users find relevant documentation\n    - Provide step-by-step guides for common issues\n    - Recommend helpful articles\n    - Answer questions using knowledge base content\n    \n    When users ask about IT issues:\n    1. Search for relevant articles\n    2. Retrieve full article content if needed\n    3. Provide clear, actionable guidance\n    4. Include article IDs for reference\n    \n    Always search the knowledge base first before answering.\n    \"\"\"\n)\n\nprint(\"✅ Knowledge Specialist Agent created!\")\nprint(f\"   Model: {OPENAI_MODEL}\")\nprint(f\"   Tools: Connected to Knowledge Base MCP Server\")\nprint(f\"   Capabilities: Search and retrieve documentation\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: System Monitoring Specialist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create MCPToolset for system monitoring\nmonitoring_toolset = MCPToolset(\n    connection_params=StdioConnectionParams(\n        server_params=StdioServerParameters(\n            command=\"python\",\n            args=[\"system_monitoring_mcp_server.py\"]\n        )\n    )\n)\n\n# Create system specialist agent\nsystem_specialist = LlmAgent(\n    model=LiteLlm(model=f\"openai/{OPENAI_MODEL}\"),\n    name=\"system_specialist\",\n    tools=[monitoring_toolset],\n    instruction=\"\"\"\n    You are a System Monitoring Specialist for IT support.\n    \n    YOUR CAPABILITIES:\n    You can monitor systems through MCP server tools:\n    - Check system health and status\n    - List all monitored systems\n    - Get detailed metrics (CPU, memory, disk, network)\n    - Ping systems to test connectivity\n    - Retrieve system logs\n    - Get active alerts\n    \n    YOUR ROLE:\n    - Monitor system health\n    - Diagnose performance issues\n    - Identify system problems\n    - Provide recommendations\n    \n    When asked about system issues:\n    1. Check system health first\n    2. Get relevant metrics if needed\n    3. Check logs for errors\n    4. Provide clear diagnosis and recommendations\n    \n    Alert Priority:\n    - Critical: Immediate action required\n    - Warning: Monitor closely, may need action\n    - Healthy: No issues\n    \"\"\"\n)\n\nprint(\"✅ System Specialist Agent created!\")\nprint(f\"   Model: {OPENAI_MODEL}\")\nprint(f\"   Tools: Connected to System Monitoring MCP Server\")\nprint(f\"   Capabilities: Health checks, metrics, logs\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎭 Part 4: Building the Complete Multi-Agent System\n",
    "\n",
    "Now let's create a coordinator agent that routes requests to our specialists!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create IT Support Coordinator with all specialists\n",
    "support_coordinator = LlmAgent(\n",
    "    model=LiteLlm(model=f\"openai/{OPENAI_MODEL}\"),\n",
    "    name=\"it_support_coordinator\",\n",
    "    sub_agents=[\n",
    "        ticket_specialist,\n",
    "        knowledge_specialist,\n",
    "        system_specialist\n",
    "    ],\n",
    "    instruction=\"\"\"\n",
    "    You are the IT Support Coordinator.\n",
    "    \n",
    "    YOUR TEAM:\n",
    "    You manage a team of specialists:\n",
    "    \n",
    "    1. **ticket_specialist**: Manages support tickets\n",
    "       - Use for: viewing, creating, updating tickets\n",
    "       - Connected to ticket database via MCP\n",
    "    \n",
    "    2. **knowledge_specialist**: Searches documentation\n",
    "       - Use for: finding guides, troubleshooting steps\n",
    "       - Connected to knowledge base via MCP\n",
    "    \n",
    "    3. **system_specialist**: Monitors system health\n",
    "       - Use for: checking servers, diagnosing issues\n",
    "       - Connected to monitoring system via MCP\n",
    "    \n",
    "    YOUR ROLE:\n",
    "    1. Understand the user's request\n",
    "    2. Route to the appropriate specialist(s)\n",
    "    3. Coordinate multi-step workflows when needed\n",
    "    4. Provide clear, helpful responses\n",
    "    \n",
    "    ROUTING GUIDELINES:\n",
    "    - Ticket questions → ticket_specialist\n",
    "    - How-to questions → knowledge_specialist\n",
    "    - System issues → system_specialist\n",
    "    - Complex issues → multiple specialists\n",
    "    \n",
    "    WORKFLOW EXAMPLES:\n",
    "    \n",
    "    User: \"What's the status of ticket T-1001?\"\n",
    "    → Route to ticket_specialist\n",
    "    \n",
    "    User: \"How do I reset my password?\"\n",
    "    → Route to knowledge_specialist\n",
    "    \n",
    "    User: \"Is the email server working?\"\n",
    "    → Route to system_specialist\n",
    "    \n",
    "    User: \"Create a ticket for the email server issue\"\n",
    "    → 1. system_specialist (check email server)\n",
    "    → 2. ticket_specialist (create ticket with details)\n",
    "    \n",
    "    Always be helpful, professional, and efficient.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(\"✅ IT Support Coordinator Agent created!\")\n",
    "print(f\"   Model: {OPENAI_MODEL}\")\n",
    "print(f\"   Sub-agents: 3 specialists\")\n",
    "print(f\"   Capabilities: Complete IT support system\")\n",
    "print(f\"\\n🏗️  Architecture:\")\n",
    "print(f\"   Coordinator (ADK) → Specialists (ADK) → MCP Servers → Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1: Setup Runner and Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create session service and runner\n",
    "session_service = InMemorySessionService()\n",
    "APP_NAME = \"it_support_mcp_system\"\n",
    "\n",
    "runner = Runner(\n",
    "    app_name=APP_NAME,\n",
    "    agent=support_coordinator,\n",
    "    session_service=session_service\n",
    ")\n",
    "\n",
    "print(\"✅ Runner initialized!\")\n",
    "print(f\"   App: {APP_NAME}\")\n",
    "print(f\"   Ready to handle IT support requests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2: Helper Function for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track active sessions\n",
    "_sessions = set()\n",
    "\n",
    "async def ask_support_async(question: str, session_id: str = \"support_session\"):\n",
    "    \"\"\"\n",
    "    Ask the IT support system a question.\n",
    "    \"\"\"\n",
    "    user_id = \"user_001\"\n",
    "    \n",
    "    # Create session if needed\n",
    "    session_key = (session_id, user_id)\n",
    "    if session_key not in _sessions:\n",
    "        await session_service.create_session(\n",
    "            app_name=APP_NAME,\n",
    "            user_id=user_id,\n",
    "            session_id=session_id,\n",
    "            state={}\n",
    "        )\n",
    "        _sessions.add(session_key)\n",
    "    \n",
    "    # Create message\n",
    "    content = types.Content(role='user', parts=[types.Part(text=question)])\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"❓ USER QUESTION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{question}\")\n",
    "    print(f\"\\n⏳ Processing (Coordinator → Specialists → MCP Servers)...\\n\")\n",
    "    \n",
    "    # Run agent\n",
    "    events = runner.run_async(user_id=user_id, session_id=session_id, new_message=content)\n",
    "    \n",
    "    # Process events\n",
    "    final_response = None\n",
    "    async for event in events:\n",
    "        if event.is_final_response():\n",
    "            final_response = event.content.parts[0].text\n",
    "    \n",
    "    if final_response:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"💬 SUPPORT RESPONSE\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(final_response)\n",
    "        print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return final_response\n",
    "\n",
    "def ask_support(question: str, session_id: str = \"support_session\"):\n",
    "    \"\"\"Synchronous wrapper.\"\"\"\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "        return asyncio.run(ask_support_async(question, session_id))\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(ask_support_async(question, session_id))\n",
    "\n",
    "print(\"✅ Helper function ready!\")\n",
    "print(\"   Use: ask_support('your question here')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎮 Part 5: Testing the Complete System\n",
    "\n",
    "Let's test our complete ADK + MCP system with real scenarios!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Ticket Management\n",
    "\n",
    "Testing the ticket specialist with MCP ticket database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask about a specific ticket\n",
    "ask_support(\"What's the status of ticket T-1001?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List high priority tickets\n",
    "ask_support(\"Show me all high priority tickets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Knowledge Base Search\n",
    "\n",
    "Testing the knowledge specialist with MCP knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for password reset guide\n",
    "ask_support(\"How do I reset my password?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for WiFi troubleshooting\n",
    "ask_support(\"My WiFi keeps disconnecting, what should I do?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: System Monitoring\n",
    "\n",
    "Testing the system specialist with MCP monitoring server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check email server health\n",
    "ask_support(\"Is the email server working? Check email-server-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all critical alerts\n",
    "ask_support(\"Show me all critical system alerts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4: Multi-Agent Workflow\n",
    "\n",
    "Testing complex workflow requiring multiple specialists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex request: check system + find docs + create ticket\n",
    "ask_support(\n",
    "    \"The file server seems slow. Can you check its status, find troubleshooting docs, \"\n",
    "    \"and create a ticket if there's an issue?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 5: Create New Ticket\n",
    "\n",
    "Testing ticket creation via MCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new ticket\n",
    "ask_support(\n",
    "    \"Create a ticket: My mouse is not working properly. It keeps freezing. \"\n",
    "    \"Priority: medium, assign to hardware team.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📊 Part 6: Architecture Overview\n",
    "\n",
    "### What Just Happened?\n",
    "\n",
    "Let's trace a request through the complete stack:\n",
    "\n",
    "**Example: \"What's the status of ticket T-1001?\"**\n",
    "\n",
    "```\n",
    "1. USER\n",
    "   ↓\n",
    "   \"What's the status of ticket T-1001?\"\n",
    "   ↓\n",
    "\n",
    "2. COORDINATOR AGENT (ADK)\n",
    "   ↓\n",
    "   Uses OpenAI (gpt-5-nano) to understand request\n",
    "   Decides: This is a ticket question\n",
    "   Routes to: ticket_specialist\n",
    "   ↓\n",
    "\n",
    "3. TICKET SPECIALIST AGENT (ADK)\n",
    "   ↓\n",
    "   Has access to ticket_toolset (MCPToolset)\n",
    "   Uses OpenAI to decide: Need to call get_ticket tool\n",
    "   ↓\n",
    "\n",
    "4. MCPTOOLSET\n",
    "   ↓\n",
    "   Sends tool call to MCP server via stdio\n",
    "   Tool: get_ticket(ticket_id=\"T-1001\")\n",
    "   ↓\n",
    "\n",
    "5. TICKET MCP SERVER\n",
    "   ↓\n",
    "   Receives call_tool request\n",
    "   Looks up ticket in database\n",
    "   Returns ticket data as JSON\n",
    "   ↓\n",
    "\n",
    "6. MCPTOOLSET ← MCP SERVER\n",
    "   ↓\n",
    "   Receives JSON response\n",
    "   Converts to ADK tool result\n",
    "   ↓\n",
    "\n",
    "7. TICKET SPECIALIST ← MCPToolset\n",
    "   ↓\n",
    "   Receives tool result\n",
    "   Uses OpenAI to format response\n",
    "   ↓\n",
    "\n",
    "8. COORDINATOR ← TICKET SPECIALIST\n",
    "   ↓\n",
    "   Receives specialist response\n",
    "   Returns to user\n",
    "   ↓\n",
    "\n",
    "9. USER ← COORDINATOR\n",
    "   ✅\n",
    "   \"Ticket T-1001: Laptop won't boot (Status: Open, Priority: High)\"\n",
    "```\n",
    "\n",
    "### Layer Responsibilities\n",
    "\n",
    "| Layer | Purpose | Technology | In Our System |\n",
    "|-------|---------|------------|---------------|\n",
    "| **Reasoning** | Understand intent, make decisions | OpenAI API | gpt-5-nano |\n",
    "| **Orchestration** | Coordinate agents, manage workflows | Google ADK | Coordinator + Specialists |\n",
    "| **Connectivity** | Standardized data access | MCP | 3 MCP servers |\n",
    "| **Data** | Actual data storage | Various | In-memory dicts (demo) |\n",
    "\n",
    "### Why This Architecture Wins\n",
    "\n",
    "✅ **Modularity**\n",
    "- MCP servers can be reused by any MCP client\n",
    "- ADK agents can use any MCP server\n",
    "- Can swap OpenAI for other providers\n",
    "\n",
    "✅ **Scalability**\n",
    "- Add new MCP servers without changing agents\n",
    "- Add new specialists without changing coordinator\n",
    "- Scale each layer independently\n",
    "\n",
    "✅ **Maintainability**\n",
    "- Clear separation of concerns\n",
    "- Update data layer without touching AI logic\n",
    "- Update AI logic without touching data layer\n",
    "\n",
    "✅ **Composability**\n",
    "- Mix and match components\n",
    "- Build complex systems from simple pieces\n",
    "- Reuse across projects\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 Part 7: Student Exercises\n",
    "\n",
    "### Exercise 1: Create a New MCP Server (Intermediate)\n",
    "\n",
    "**Task:** Create a simple User Directory MCP server.\n",
    "\n",
    "**Requirements:**\n",
    "1. Create `user_directory_mcp_server.py`\n",
    "2. Store employee data (name, email, department, phone)\n",
    "3. Implement tools:\n",
    "   - `search_employee`: Find by name\n",
    "   - `get_employee`: Get by email\n",
    "   - `list_by_department`: Filter by department\n",
    "4. Follow MCP server pattern from existing servers\n",
    "\n",
    "**Bonus:** Connect it to the coordinator agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "\n",
    "# TODO: Create user_directory_mcp_server.py\n",
    "# TODO: Start the server\n",
    "# TODO: Create MCPToolset for user directory\n",
    "# TODO: Create a user_directory_specialist agent\n",
    "# TODO: Add to coordinator's sub_agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Build a Report Generator (Advanced)\n",
    "\n",
    "**Task:** Create an agent that generates IT support reports.\n",
    "\n",
    "**Requirements:**\n",
    "1. Create a `report_generator` agent\n",
    "2. Access multiple MCP servers (tickets + systems)\n",
    "3. Generate reports:\n",
    "   - Open tickets summary\n",
    "   - Critical system alerts\n",
    "   - Team workload distribution\n",
    "4. Format output nicely\n",
    "\n",
    "**Hint:** Use multiple MCPToolsets in one agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "\n",
    "# TODO: Create report generator agent with multiple toolsets\n",
    "# report_agent = LlmAgent(\n",
    "#     model=LiteLlm(model=f\"openai/{OPENAI_MODEL}\"),\n",
    "#     name=\"report_generator\",\n",
    "#     tools=[ticket_toolset, monitoring_toolset],  # Multiple MCP servers!\n",
    "#     instruction=\"...\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Implement Auto-Escalation (Advanced)\n",
    "\n",
    "**Task:** Create workflow that auto-escalates critical issues.\n",
    "\n",
    "**Requirements:**\n",
    "1. Monitor for critical system alerts\n",
    "2. Automatically create high-priority tickets\n",
    "3. Search knowledge base for relevant docs\n",
    "4. Include all info in ticket description\n",
    "\n",
    "**Hint:** Use SequentialAgent or create custom workflow logic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "\n",
    "# TODO: Create auto-escalation workflow\n",
    "# Steps:\n",
    "# 1. Check for critical alerts (system_specialist)\n",
    "# 2. For each alert, search KB (knowledge_specialist)\n",
    "# 3. Create ticket with alert + KB info (ticket_specialist)\n",
    "# 4. Consider using SequentialAgent or LoopAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎯 Part 8: Course Summary - Your Journey\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "**Lesson 1: Foundations**\n",
    "- Basic LLM agents with personality\n",
    "- OpenAI integration via LiteLLM\n",
    "- Session management\n",
    "\n",
    "**Lesson 2: Tools**\n",
    "- Function calling\n",
    "- Custom tool development\n",
    "- Tool best practices\n",
    "\n",
    "**Lesson 3: Multi-Agent Coordination**\n",
    "- Hierarchical routing\n",
    "- Coordinator + specialists pattern\n",
    "- sub_agents parameter\n",
    "\n",
    "**Lesson 4: Workflow Patterns**\n",
    "- SequentialAgent (pipelines)\n",
    "- ParallelAgent (concurrent execution)\n",
    "- Performance optimization\n",
    "\n",
    "**Lesson 5: Advanced Patterns**\n",
    "- Rule-based agents (BaseAgent)\n",
    "- Hybrid architectures\n",
    "- LoopAgent\n",
    "- Cost optimization (70% savings!)\n",
    "\n",
    "**Lesson 6: Complete Stack**\n",
    "- MCP server development\n",
    "- ADK + MCP integration\n",
    "- Production architecture\n",
    "- Composable systems\n",
    "\n",
    "### The Complete AI Agent Stack\n",
    "\n",
    "You now understand how to build production AI systems:\n",
    "\n",
    "```\n",
    "🧠 REASONING LAYER\n",
    "   OpenAI / Anthropic / Google / etc.\n",
    "   ↕️\n",
    "🎭 ORCHESTRATION LAYER\n",
    "   Google ADK (or other frameworks)\n",
    "   ↕️\n",
    "🔌 CONNECTIVITY LAYER\n",
    "   MCP (standardized protocol)\n",
    "   ↕️\n",
    "💾 DATA LAYER\n",
    "   Databases / APIs / Files / etc.\n",
    "```\n",
    "\n",
    "### Design Patterns You've Mastered\n",
    "\n",
    "| Pattern | Use Case | Lesson |\n",
    "|---------|----------|--------|\n",
    "| **Basic Agent** | Single-purpose tasks | 1 |\n",
    "| **Tool-equipped Agent** | Actions on external systems | 2 |\n",
    "| **Hierarchical Routing** | Complex triage and routing | 3 |\n",
    "| **Sequential Workflow** | Pipeline processing | 4 |\n",
    "| **Parallel Execution** | Concurrent operations | 4 |\n",
    "| **Rule-Based Agent** | Deterministic logic | 5 |\n",
    "| **Hybrid Architecture** | Cost-optimized systems | 5 |\n",
    "| **Loop Agent** | Iterative workflows | 5 |\n",
    "| **MCP Integration** | Data connectivity | 6 |\n",
    "\n",
    "### Production Best Practices\n",
    "\n",
    "✅ **Architecture**\n",
    "- Separate concerns (reasoning, orchestration, data)\n",
    "- Use MCP for data connectivity\n",
    "- Design for composability\n",
    "\n",
    "✅ **Cost Optimization**\n",
    "- Rules first, LLM second (70% savings)\n",
    "- Right-size models (gpt-5-nano vs gpt-4o)\n",
    "- Cache when possible\n",
    "\n",
    "✅ **Performance**\n",
    "- Parallel execution when possible\n",
    "- Async/await patterns\n",
    "- Background processes for MCP servers\n",
    "\n",
    "✅ **Maintainability**\n",
    "- Clear agent responsibilities\n",
    "- Reusable MCP servers\n",
    "- Comprehensive instructions\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Immediate Next Steps:**\n",
    "1. 🏗️ Build your own MCP servers for your use case\n",
    "2. 🤖 Create specialized agents\n",
    "3. 🔗 Connect them with ADK\n",
    "4. 🚀 Deploy to production!\n",
    "\n",
    "**Advanced Topics to Explore:**\n",
    "- Persistent state (Firestore, PostgreSQL)\n",
    "- Authentication and authorization\n",
    "- Rate limiting and quotas\n",
    "- Monitoring and observability\n",
    "- Error handling and retries\n",
    "- A2A (Agent-to-Agent) protocol\n",
    "- Production deployment (Cloud Run, Kubernetes)\n",
    "\n",
    "### Resources\n",
    "\n",
    "**ADK Documentation:**\n",
    "- [ADK Official Docs](https://google.github.io/adk-docs/)\n",
    "- [ADK MCP Integration](https://google.github.io/adk-docs/tools/mcp-tools/)\n",
    "- [ADK Examples](https://github.com/google/adk-examples)\n",
    "\n",
    "**MCP Documentation:**\n",
    "- [MCP Specification](https://modelcontextprotocol.io/)\n",
    "- [MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk)\n",
    "- [MCP Servers List](https://github.com/modelcontextprotocol/servers)\n",
    "\n",
    "**Community:**\n",
    "- [ADK Discord](https://discord.gg/google-adk)\n",
    "- [MCP Community](https://github.com/modelcontextprotocol/community)\n",
    "\n",
    "---\n",
    "\n",
    "## 🎉 Congratulations!\n",
    "\n",
    "You've completed the complete ADK tutorial series!\n",
    "\n",
    "You can now:\n",
    "- ✅ Build multi-agent AI systems\n",
    "- ✅ Connect agents to real data via MCP\n",
    "- ✅ Design production-grade architectures\n",
    "- ✅ Optimize for cost and performance\n",
    "- ✅ Deploy the complete AI stack\n",
    "\n",
    "**You're ready to build production AI systems! 🚀**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧹 Part 9: Cleanup\n",
    "\n",
    "Don't forget to stop MCP servers when done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop all MCP servers\n",
    "server_manager.stop_all()\n",
    "\n",
    "print(\"\\n✅ Cleanup complete!\")\n",
    "print(\"   All MCP servers stopped\")\n",
    "print(\"   Session data cleared\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}